#!/usr/bin/env python3
"""
æ¯æ—¥ä¿¡æ¯æ”¶é›†è„šæœ¬ - OpenClaw ç‰ˆæœ¬ V2
åŸºäº daily_info_collection é¡¹ç›®æ¶æ„
æ•°æ®æºï¼šGitHub Trending + Hacker News + Reddit + æŠ€æœ¯èµ„è®¯
"""

import requests
import json
from datetime import datetime
import os
import time
import re
from typing import Dict, List, Any, Optional


class DailyInfoCollectorV2:
    """æ¯æ—¥ä¿¡æ¯æ”¶é›†å™¨ V2 - å›½é™…ç§‘æŠ€èµ„è®¯ä¸ºä¸»"""
    
    def __init__(self, output_dir: str = None):
        self.date = datetime.now().strftime('%Y-%m-%d')
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = output_dir or f"/root/.openclaw/workspace/reports/daily-info/{self.date}"
        self.results = []
        self.valid_items = []
    
    def prepare_output_directory(self):
        os.makedirs(self.output_dir, exist_ok=True)
    
    def normalize_common(self, title: str, url: str, source: str, category: str, 
                        platform: str, rank: int = 0, **kwargs) -> Dict:
        """é€šç”¨æ•°æ®æ ‡å‡†åŒ–"""
        return {
            "id": kwargs.get('id') or f"{platform}_{rank}",
            "title": title,
            "content": {
                "summary": kwargs.get('summary') or kwargs.get('description'),
                "full_text": None
            },
            "media": {
                "image": {"url": kwargs.get('image_url'), "thumbnail": None},
                "video": None
            },
            "links": {
                "main": url,
                "mobile": None,
                "share": kwargs.get('share_url')
            },
            "metrics": {
                "hot": {"value": kwargs.get('stars') or kwargs.get('score'), "label": kwargs.get('metric_label', 'çƒ­åº¦')},
                "views": kwargs.get('views'),
                "interactions": {
                    "likes": kwargs.get('likes'),
                    "comments": kwargs.get('comments'),
                    "shares": kwargs.get('shares')
                }
            },
            "author": {
                "name": kwargs.get('author'),
                "avatar": kwargs.get('author_avatar')
            },
            "time": {
                "published": kwargs.get('published_at'),
                "collected": datetime.now().isoformat()
            },
            "category": category,
            "tags": kwargs.get('tags', []),
            "meta": {
                "source": source,
                "platform": platform,
                "category": category,
                "rank": rank,
                "collection_time": datetime.now().isoformat()
            }
        }
    
    def fetch_github_trending(self) -> Dict:
        """è·å– GitHub Trending (ä½¿ç”¨ search API)"""
        print("  Fetching GitHub Trending...", end=' ')
        
        try:
            # è·å–æœ€è¿‘ä¸€å‘¨åˆ›å»ºçš„ä¼˜è´¨ä»“åº“
            one_week_ago = (datetime.now() - __import__('datetime').timedelta(days=7)).strftime('%Y-%m-%d')
            url = f"https://api.github.com/search/repositories?q=created:>{one_week_ago}&sort=stars&order=desc&per_page=20"
            
            response = requests.get(url, timeout=30)
            if response.status_code != 200:
                print(f"FAILED (HTTP {response.status_code})")
                return self._error_result("GitHub Trending", "tech", "github", f"HTTP {response.status_code}")
            
            data = response.json()
            items = []
            
            for rank, repo in enumerate(data.get('items', [])[:20], 1):
                item = self.normalize_common(
                    title=repo.get('name', ''),
                    url=repo.get('html_url', ''),
                    source="GitHub Trending",
                    category="tech",
                    platform="github",
                    rank=rank,
                    id=str(repo.get('id')),
                    summary=repo.get('description'),
                    author=repo.get('owner', {}).get('login'),
                    author_avatar=repo.get('owner', {}).get('avatar_url'),
                    stars=repo.get('stargazers_count'),
                    metric_label="Stars",
                    tags=repo.get('topics', []),
                    published_at=repo.get('created_at')
                )
                items.append(item)
            
            print(f"OK ({len(items)} repos)")
            return self._success_result("GitHub Trending", "tech", "github", len(items), items)
            
        except Exception as e:
            print(f"ERROR ({str(e)})")
            return self._error_result("GitHub Trending", "tech", "github", str(e))
    
    def fetch_hackernews(self) -> Dict:
        """è·å– Hacker News çƒ­é—¨"""
        print("  Fetching Hacker News...", end=' ')
        
        try:
            # å…ˆè·å– Top Stories ID åˆ—è¡¨
            top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(top_url, timeout=30)
            
            if response.status_code != 200:
                print(f"FAILED (HTTP {response.status_code})")
                return self._error_result("Hacker News", "tech", "hackernews", f"HTTP {response.status_code}")
            
            story_ids = response.json()[:20]  # å–å‰20ä¸ª
            items = []
            
            for rank, story_id in enumerate(story_ids, 1):
                try:
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_resp = requests.get(story_url, timeout=10)
                    
                    if story_resp.status_code == 200:
                        story = story_resp.json()
                        if story:
                            item = self.normalize_common(
                                title=story.get('title', ''),
                                url=story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                                source="Hacker News",
                                category="tech",
                                platform="hackernews",
                                rank=rank,
                                id=str(story_id),
                                summary=None,
                                author=story.get('by'),
                                score=story.get('score'),
                                metric_label="Points",
                                comments=story.get('descendants')
                            )
                            items.append(item)
                    time.sleep(0.1)  # ç¤¼è²Œå»¶è¿Ÿ
                except:
                    continue
            
            print(f"OK ({len(items)} stories)")
            return self._success_result("Hacker News", "tech", "hackernews", len(items), items)
            
        except Exception as e:
            print(f"ERROR ({str(e)})")
            return self._error_result("Hacker News", "tech", "hackernews", str(e))
    
    def fetch_producthunt(self) -> Dict:
        """è·å– Product Hunt çƒ­é—¨ï¼ˆé€šè¿‡éå®˜æ–¹æ–¹å¼ï¼‰"""
        print("  Fetching Product Hunt...", end=' ')
        
        try:
            # Product Hunt éœ€è¦è®¤è¯ï¼Œè¿™é‡Œå°è¯•è·å–ä»Šæ—¥çƒ­é—¨ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼‰
            url = "https://www.producthunt.com/feed"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                # ç®€åŒ–å¤„ç† - å®é™…ä¸Šéœ€è¦è§£æ HTML
                print("OK (fallback)")
                return self._success_result("Product Hunt", "tech", "producthunt", 0, [])
            else:
                print(f"FAILED (HTTP {response.status_code})")
                return self._error_result("Product Hunt", "tech", "producthunt", f"HTTP {response.status_code}")
                
        except Exception as e:
            print(f"ERROR ({str(e)})")
            return self._error_result("Product Hunt", "tech", "producthunt", str(e))
    
    def fetch_devto(self) -> Dict:
        """è·å– Dev.to çƒ­é—¨æ–‡ç« """
        print("  Fetching Dev.to...", end=' ')
        
        try:
            url = "https://dev.to/api/articles?per_page=20&top=7"
            response = requests.get(url, timeout=30)
            
            if response.status_code != 200:
                print(f"FAILED (HTTP {response.status_code})")
                return self._error_result("Dev.to", "tech", "devto", f"HTTP {response.status_code}")
            
            articles = response.json()
            items = []
            
            for rank, article in enumerate(articles[:20], 1):
                item = self.normalize_common(
                    title=article.get('title', ''),
                    url=article.get('url', ''),
                    source="Dev.to",
                    category="tech",
                    platform="devto",
                    rank=rank,
                    id=str(article.get('id')),
                    summary=article.get('description'),
                    author=article.get('user', {}).get('name'),
                    author_avatar=article.get('user', {}).get('profile_image'),
                    likes=article.get('positive_reactions_count'),
                    comments=article.get('comments_count'),
                    tags=article.get('tag_list', []),
                    published_at=article.get('published_at')
                )
                items.append(item)
            
            print(f"OK ({len(items)} articles)")
            return self._success_result("Dev.to", "tech", "devto", len(items), items)
            
        except Exception as e:
            print(f"ERROR ({str(e)})")
            return self._error_result("Dev.to", "tech", "devto", str(e))
    
    def _success_result(self, source, category, platform, count, items):
        return {
            "source": source,
            "category": category,
            "platform": platform,
            "status": "success",
            "count": count,
            "valid_count": len(items),
            "items": items
        }
    
    def _error_result(self, source, category, platform, error):
        return {
            "source": source,
            "category": category,
            "platform": platform,
            "status": "error",
            "error": str(error),
            "count": 0,
            "valid_count": 0,
            "items": []
        }
    
    def collect_all(self):
        """æ”¶é›†æ‰€æœ‰æ•°æ®æº"""
        self.prepare_output_directory()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“° Daily Info Collector V2 - {self.date}")
        print(f"{'='*60}\n")
        
        # æ‰§è¡Œå„æ•°æ®æºæ”¶é›†
        collectors = [
            self.fetch_github_trending,
            self.fetch_hackernews,
            self.fetch_devto,
            # self.fetch_producthunt  # å¯èƒ½éœ€è¦è®¤è¯ï¼Œæš‚æ—¶è·³è¿‡
        ]
        
        for collector in collectors:
            result = collector()
            self.results.append(result)
            self.valid_items.extend(result.get('items', []))
            time.sleep(1)
        
        return self.save_and_summarize()
    
    def save_and_summarize(self):
        """ä¿å­˜ç»“æœå¹¶ç”Ÿæˆæ±‡æ€»"""
        total_items = len(self.valid_items)
        successful = len([r for r in self.results if r['status'] == 'success'])
        failed = len([r for r in self.results if r['status'] == 'error'])
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for item in self.valid_items:
            cat = item.get('category', 'unknown')
            if cat not in categories:
                categories[cat] = {'count': 0, 'sources': set()}
            categories[cat]['count'] += 1
            categories[cat]['sources'].add(item.get('meta', {}).get('platform', ''))
        
        # æ„å»ºåˆ†ç±»ç»Ÿè®¡
        category_summary = {}
        for cat, stats in categories.items():
            category_summary[cat] = {
                'count': stats['count'],
                'sources': list(stats['sources'])
            }
        
        output_data = {
            "schema_version": "2.0",
            "date": self.date,
            "collection_time": datetime.now().isoformat(),
            "summary": {
                "total_items": total_items,
                "total_sources": len(self.results),
                "successful_sources": successful,
                "failed_sources": failed,
                "categories": category_summary
            },
            "items": self.valid_items
        }
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        detail_file = os.path.join(self.output_dir, f"daily_info_{self.timestamp}.json")
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ latest.json
        latest_json = os.path.join(self.output_dir, "latest.json")
        with open(latest_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_content = self.generate_markdown_report(output_data)
        md_file = os.path.join(self.output_dir, f"daily_info_{self.timestamp}.md")
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # ä¿å­˜ latest.md
        latest_md = os.path.join(self.output_dir, "latest.md")
        with open(latest_md, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"\n{'='*60}")
        print(f"âœ… Collection Complete")
        print(f"{'='*60}")
        print(f"Total Items: {total_items}")
        print(f"Sources: {successful}/{len(self.results)} successful")
        print(f"Output: {detail_file}")
        print(f"{'='*60}\n")
        
        return {
            "date": self.date,
            "total_items": total_items,
            "successful_sources": successful,
            "failed_sources": failed,
            "output_files": {
                "json": detail_file,
                "markdown": md_file,
                "latest_json": latest_json,
                "latest_md": latest_md
            },
            "summary": output_data["summary"]
        }
    
    def generate_markdown_report(self, data: Dict) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        lines = [
            f"# ğŸ“° æ¯æ—¥ä¿¡æ¯æ”¶é›†æŠ¥å‘Š - {self.date}",
            "",
            f"**æ”¶é›†æ—¶é—´**: {data['collection_time']}",
            f"**æ•°æ®ç‰ˆæœ¬**: Schema {data['schema_version']}",
            "",
            "## ğŸ“Š æ±‡æ€»ç»Ÿè®¡",
            "",
            f"- **æ€»æ¡ç›®æ•°**: {data['summary']['total_items']}",
            f"- **æ•°æ®æº**: {data['summary']['successful_sources']}/{data['summary']['total_sources']} æˆåŠŸ",
            "",
            "### æŒ‰æ¥æºç»Ÿè®¡",
            ""
        ]
        
        # æŒ‰æ¥æºç»Ÿè®¡
        for result in self.results:
            if result['status'] == 'success':
                emoji = {"github": "â­", "hackernews": "ğŸ“°", "devto": "ğŸ’»", "producthunt": "ğŸš€"}.get(result['platform'], "ğŸ“„")
                lines.append(f"- {emoji} **{result['source']}**: {result['valid_count']} æ¡")
        
        lines.extend(["", "## ğŸ“‘ çƒ­é—¨å†…å®¹", ""])
        
        # GitHub Trending
        github_items = [i for i in data['items'] if i.get('meta', {}).get('platform') == 'github'][:10]
        if github_items:
            lines.extend(["### â­ GitHub Trending", ""])
            for i, item in enumerate(github_items, 1):
                title = item.get('title', 'N/A')
                url = item.get('links', {}).get('main', '')
                stars = item.get('metrics', {}).get('hot', {}).get('value', '')
                summary = item.get('content', {}).get('summary', '')
                lines.append(f"{i}. **[{title}]({url})** â­{stars}")
                if summary:
                    lines.append(f"   > {summary[:100]}..." if len(summary) > 100 else f"   > {summary}")
                lines.append("")
        
        # Hacker News
        hn_items = [i for i in data['items'] if i.get('meta', {}).get('platform') == 'hackernews'][:10]
        if hn_items:
            lines.extend(["### ğŸ“° Hacker News", ""])
            for i, item in enumerate(hn_items, 1):
                title = item.get('title', 'N/A')
                url = item.get('links', {}).get('main', '')
                score = item.get('metrics', {}).get('hot', {}).get('value', '')
                comments = item.get('metrics', {}).get('interactions', {}).get('comments', '')
                comment_str = f" ğŸ’¬{comments}" if comments else ""
                lines.append(f"{i}. [{title}]({url}) ğŸ”º{score}{comment_str}")
            lines.append("")
        
        # Dev.to
        devto_items = [i for i in data['items'] if i.get('meta', {}).get('platform') == 'devto'][:10]
        if devto_items:
            lines.extend(["### ğŸ’» Dev.to çƒ­é—¨", ""])
            for i, item in enumerate(devto_items, 1):
                title = item.get('title', 'N/A')
                url = item.get('links', {}).get('main', '')
                likes = item.get('metrics', {}).get('interactions', {}).get('likes', '')
                like_str = f" â¤ï¸{likes}" if likes else ""
                lines.append(f"{i}. [{title}]({url}){like_str}")
            lines.append("")
        
        lines.extend(["---", "", "*Generated by OpenClaw Daily Info Collector V2*"])
        return "\n".join(lines)


def main():
    collector = DailyInfoCollectorV2()
    result = collector.collect_all()
    
    # è¾“å‡ºç®€è¦æ±‡æŠ¥
    print("\n" + "="*60)
    print("BRIEF_REPORT_START")
    print(json.dumps({
        "date": result["date"],
        "total_items": result["total_items"],
        "sources_success": result["successful_sources"],
        "sources_failed": result["failed_sources"],
        "output_dir": collector.output_dir
    }, ensure_ascii=False))
    print("BRIEF_REPORT_END")
    print("="*60)


if __name__ == "__main__":
    main()
