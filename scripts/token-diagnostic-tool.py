#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ Token æ”¶é›†å’Œè¯Šæ–­å·¥å…·
ç”¨äºè§£å†³ token_monitor ä¸å·¥ä½œã€token_stats ç»Ÿè®¡ç¼ºå¤±çš„é—®é¢˜
"""

import os
import json
import glob
import argparse
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set

class TokenDiagnosticTool:
    def __init__(self):
        self.sessions_dir = Path("/root/.openclaw/agents/main/sessions")
        self.token_usage_file = Path("/root/.openclaw/workspace/memory/token-usage.jsonl")
        self.models_config = Path("/root/.openclaw/openclaw.json")
        
    def diagnose_all_sessions(self) -> Dict:
        """è¯Šæ–­æ‰€æœ‰ session æ–‡ä»¶"""
        print("ğŸ” å¼€å§‹è¯Šæ–­æ‰€æœ‰ Session æ–‡ä»¶...")
        
        results = {
            "total_sessions": 0,
            "sessions_with_tokens": 0,
            "missing_token_data": [],
            "models_found": set(),
            "providers_found": set(),
            "issues_found": []
        }
        
        session_files = list(self.sessions_dir.glob("*.jsonl"))
        results["total_sessions"] = len(session_files)
        
        for session_file in session_files:
            try:
                with open(session_file, 'r', encoding='utf-8') as f:
                    session_data = []
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                entry = json.loads(line)
                                session_data.append(entry)
                            except json.JSONDecodeError:
                                results["issues_found"].append(f"JSONè§£æé”™è¯¯: {session_file}")
                                continue
                
                # æ£€æŸ¥ token æ•°æ®
                has_tokens = self._check_session_tokens(session_data)
                if has_tokens:
                    results["sessions_with_tokens"] += 1
                    
                    # æå–æ¨¡å‹ä¿¡æ¯
                    models, providers = self._extract_model_info(session_data)
                    results["models_found"].update(models)
                    results["providers_found"].update(providers)
                else:
                    results["missing_token_data"].append({
                        "file": str(session_file),
                        "timestamp": self._get_session_timestamp(session_data)
                    })
                    
            except Exception as e:
                results["issues_found"].append(f"æ–‡ä»¶è¯»å–é”™è¯¯ {session_file}: {str(e)}")
        
        return results
    
    def _check_session_tokens(self, session_data: List) -> bool:
        """æ£€æŸ¥ session æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ token æ•°æ®"""
        for entry in session_data:
            if entry.get("type") == "message" and "message" in entry:
                message = entry["message"]
                if message.get("role") == "assistant" and "usage" in message:
                    usage = message["usage"]
                    if any(key in usage for key in ["input", "output", "totalTokens"]):
                        return True
        return False
    
    def _extract_model_info(self, session_data: List) -> tuple:
        """æå–æ¨¡å‹å’Œæä¾›è€…ä¿¡æ¯"""
        models = set()
        providers = set()
        
        for entry in session_data:
            if entry.get("type") == "message" and "message" in entry:
                message = entry["message"]
                provider = message.get("provider")
                model = message.get("model")
                
                if provider:
                    providers.add(provider)
                if model:
                    models.add(model)
        
        return models, providers
    
    def _get_session_timestamp(self, session_data: List) -> Optional[str]:
        """è·å– session æ—¶é—´æˆ³"""
        for entry in session_data:
            if entry.get("type") == "session":
                return entry.get("timestamp")
        return None
    
    def diagnose_token_logger(self) -> Dict:
        """è¯Šæ–­ token-logger é…ç½®"""
        print("ğŸ” æ£€æŸ¥ token-logger é…ç½®...")
        
        results = {
            "token_logger_enabled": False,
            "config_status": {},
            "hook_issues": []
        }
        
        try:
            # è¯»å–é…ç½®æ–‡ä»¶
            if self.models_config.exists():
                with open(self.models_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                hooks = config.get("hooks", {}).get("internal", {})
                entries = hooks.get("entries", {})
                
                token_logger = entries.get("token-logger", {})
                results["token_logger_enabled"] = token_logger.get("enabled", False)
                results["config_status"]["token_logger"] = token_logger
                
                if not results["token_logger_enabled"]:
                    results["hook_issues"].append("token-logger hook æœªå¯ç”¨")
            
        except Exception as e:
            results["hook_issues"].append(f"é…ç½®è¯»å–é”™è¯¯: {str(e)}")
        
        return results
    
    def diagnose_token_usage_file(self) -> Dict:
        """è¯Šæ–­ token-usage.jsonl æ–‡ä»¶"""
        print("ğŸ” æ£€æŸ¥ token-usage.jsonl æ–‡ä»¶...")
        
        results = {
            "file_exists": False,
            "entries_count": 0,
            "file_size": 0,
            "models_found": set(),
            "date_range": {"start": None, "end": None},
            "issues": []
        }
        
        if self.token_usage_file.exists():
            results["file_exists"] = True
            results["file_size"] = self.token_usage_file.stat().st_size
            
            try:
                with open(self.token_usage_file, 'r', encoding='utf-8') as f:
                    entries = []
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                entry = json.loads(line)
                                entries.append(entry)
                                
                                # æå–æ¨¡å‹ä¿¡æ¯
                                model = entry.get("model")
                                if model:
                                    results["models_found"].add(model)
                                
                                # æå–æ—¶é—´ä¿¡æ¯
                                timestamp = entry.get("timestamp")
                                if timestamp:
                                    if not results["date_range"]["start"] or timestamp < results["date_range"]["start"]:
                                        results["date_range"]["start"] = timestamp
                                    if not results["date_range"]["end"] or timestamp > results["date_range"]["end"]:
                                        results["date_range"]["end"] = timestamp
                                        
                            except json.JSONDecodeError:
                                results["issues"].append("JSON è§£æé”™è¯¯")
                                continue
                
                results["entries_count"] = len(entries)
                
            except Exception as e:
                results["issues"].append(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        else:
            results["issues"].append("token-usage.jsonl æ–‡ä»¶ä¸å­˜åœ¨")
        
        return results
    
    def diagnose_models_config(self) -> Dict:
        """è¯Šæ–­æ¨¡å‹é…ç½®"""
        print("ğŸ” æ£€æŸ¥æ¨¡å‹é…ç½®...")
        
        results = {
            "auth_profiles": {},
            "missing_providers": [],
            "models_configured": set()
        }
        
        try:
            if self.models_config.exists():
                with open(self.models_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                auth = config.get("auth", {})
                profiles = auth.get("profiles", {})
                
                results["auth_profiles"] = profiles
                
                for profile_id, profile in profiles.items():
                    provider = profile.get("provider")
                    if provider:
                        results["models_configured"].add(provider)
                
                # æ£€æŸ¥å¸¸è§æä¾›å•†æ˜¯å¦ç¼ºå¤±
                expected_providers = ["moonshot", "zai", "openai"]
                for provider in expected_providers:
                    if provider not in results["models_configured"]:
                        results["missing_providers"].append(provider)
                        
        except Exception as e:
            results["error"] = str(e)
        
        return results
    
    def run_comprehensive_diagnosis(self):
        """è¿è¡Œå…¨é¢è¯Šæ–­"""
        print("ğŸš€ å¼€å§‹å…¨é¢è¯Šæ–­ Token æ”¶é›†ç³»ç»Ÿ...")
        print("=" * 60)
        
        # æ‰§è¡Œæ‰€æœ‰è¯Šæ–­
        session_results = self.diagnose_all_sessions()
        token_logger_results = self.diagnose_token_logger()
        token_usage_results = self.diagnose_token_usage_file()
        models_results = self.diagnose_models_config()
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“Š è¯Šæ–­æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"\nğŸ“ Session æ–‡ä»¶è¯Šæ–­:")
        print(f"  â€¢ æ€»ä¼šè¯æ•°: {session_results['total_sessions']}")
        print(f"  â€¢ åŒ…å« token æ•°æ®çš„ä¼šè¯: {session_results['sessions_with_tokens']}")
        print(f"  â€¢ ç¼ºå¤± token æ•°æ®çš„ä¼šè¯: {len(session_results['missing_token_data'])}")
        
        print(f"\nğŸ·ï¸  å‘ç°çš„æ¨¡å‹å’Œæä¾›å•†:")
        print(f"  â€¢ æ¨¡å‹: {sorted(session_results['models_found'])}")
        print(f"  â€¢ æä¾›å•†: {sorted(session_results['providers_found'])}")
        
        print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for issue in session_results['issues_found']:
            print(f"  â€¢ {issue}")
        
        print(f"\nğŸ”§ Token-Logger é…ç½®:")
        print(f"  â€¢ æ˜¯å¦å¯ç”¨: {'âœ…' if token_logger_results['token_logger_enabled'] else 'âŒ'}")
        if not token_logger_results['token_logger_enabled']:
            print(f"  â€¢ é—®é¢˜: {token_logger_results['hook_issues']}")
        
        print(f"\nğŸ“Š Token-Usage æ–‡ä»¶:")
        print(f"  â€¢ æ–‡ä»¶å­˜åœ¨: {'âœ…' if token_usage_results['file_exists'] else 'âŒ'}")
        if token_usage_results['file_exists']:
            print(f"  â€¢ æ¡ç›®æ•°é‡: {token_usage_results['entries_count']}")
            print(f"  â€¢ æ–‡ä»¶å¤§å°: {token_usage_results['file_size']} bytes")
            print(f"  â€¢ æ¨¡å‹è¦†ç›–: {sorted(token_usage_results['models_found'])}")
            print(f"  â€¢ æ—¶é—´èŒƒå›´: {token_usage_results['date_range']}")
        
        print(f"\nğŸ” è®¤è¯é…ç½®:")
        print(f"  â€¢ é…ç½®çš„æä¾›å•†: {sorted(models_results['models_configured'])}")
        if models_results['missing_providers']:
            print(f"  â€¢ ç¼ºå¤±çš„æä¾›å•†: {models_results['missing_providers']}")
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        print("\nğŸ”§ ä¿®å¤å»ºè®®:")
        print("=" * 60)
        
        if not token_logger_results['token_logger_enabled']:
            print("1ï¸âƒ£ ä¿®å¤ token-logger hook:")
            print("   ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¯ç”¨ token-logger:")
            print("   ```json")
            print("   {")
            print('     "hooks": {')
            print('       "internal": {')
            print('         "enabled": true,')
            print('         "entries": {')
            print('           "token-logger": {')
            print('             "enabled": true')
            print('           }')
            print('         }')
            print('       }')
            print('     }')
            print("   }")
            print("   ```")
        
        if models_results['missing_providers']:
            print(f"\n2ï¸âƒ£ æ·»åŠ ç¼ºå¤±çš„æä¾›å•†é…ç½®:")
            for provider in models_results['missing_providers']:
                if provider == "moonshot":
                    print(f"   â€¢ {provider} (Kimi): éœ€è¦æ·»åŠ  MOONSHOT_API_KEY")
                elif provider == "zai":
                    print(f"   â€¢ {provider}: éœ€è¦æ·»åŠ  ZAI_API_KEY")
                elif provider == "openai":
                    print(f"   â€¢ {provider}: éœ€è¦æ·»åŠ  OPENAI_API_KEY")
        
        if len(session_results['missing_token_data']) > 0:
            print(f"\n3ï¸âƒ£ æ‰‹åŠ¨æ”¶é›†ç¼ºå¤±çš„ token æ•°æ®:")
            print("   è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰‹åŠ¨å¤„ç† session æ–‡ä»¶:")
            print("   python3 /root/.openclaw/workspace/skills/token-monitor/scripts/token_stats.py")
        
        print(f"\n4ï¸âƒ£ åˆ›å»ºè‡ªåŠ¨ä¿®å¤è„šæœ¬:")
        print("   å¯ä»¥åŸºäºè¯Šæ–­ç»“æœç”Ÿæˆè‡ªåŠ¨ä¿®å¤è„šæœ¬")
        
        # ä¿å­˜è¯Šæ–­ç»“æœ
        diagnosis_report = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "session_diagnosis": session_results,
            "token_logger_diagnosis": token_logger_results,
            "token_usage_diagnosis": token_usage_results,
            "models_diagnosis": models_results,
            "summary": {
                "total_sessions": session_results['total_sessions'],
                "sessions_with_tokens": session_results['sessions_with_tokens'],
                "token_logger_enabled": token_logger_results['token_logger_enabled'],
                "token_usage_exists": token_usage_results['file_exists'],
                "missing_providers": models_results['missing_providers'],
                "issues_count": len(session_results['issues_found']) + len(token_logger_results['hook_issues']) + len(token_usage_results['issues'])
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶ (è½¬æ¢setsä¸ºlists)
        report_file = Path("/root/.openclaw/workspace/reports/token-diagnosis-report.json")
        
        # è½¬æ¢setsä¸ºlists
        serializable_report = {
            "timestamp": diagnosis_report["timestamp"],
            "session_diagnosis": {
                k: (list(v) if isinstance(v, set) else v) 
                for k, v in diagnosis_report["session_diagnosis"].items()
            },
            "token_logger_diagnosis": diagnosis_report["token_logger_diagnosis"],
            "token_usage_diagnosis": {
                k: (list(v) if isinstance(v, set) else v) 
                for k, v in diagnosis_report["token_usage_diagnosis"].items()
            },
            "models_diagnosis": {
                k: (list(v) if isinstance(v, set) else v) 
                for k, v in diagnosis_report["models_diagnosis"].items()
            },
            "summary": {
                k: (list(v) if isinstance(v, set) else v) 
                for k, v in diagnosis_report["summary"].items()
            }
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return diagnosis_report

def main():
    parser = argparse.ArgumentParser(description="Token æ”¶é›†ç³»ç»Ÿè¯Šæ–­å·¥å…·")
    parser.add_argument("--fix", action="store_true", help="å°è¯•è‡ªåŠ¨ä¿®å¤å‘ç°çš„é—®é¢˜")
    parser.add_argument("--output", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    tool = TokenDiagnosticTool()
    
    if args.fix:
        print("âš ï¸  è‡ªåŠ¨ä¿®å¤åŠŸèƒ½æš‚æœªå®ç°")
        print("   è¯·å…ˆè¿è¡Œè¯Šæ–­æŸ¥çœ‹é—®é¢˜ï¼Œç„¶åæ‰‹åŠ¨ä¿®å¤")
    
    # è¿è¡Œè¯Šæ–­
    report = tool.run_comprehensive_diagnosis()
    
    # ç®€è¦æ€»ç»“
    summary = report["summary"]
    print(f"\nğŸ“ˆ è¯Šæ–­æ€»ç»“:")
    print(f"  â€¢ ä¼šè¯è¦†ç›–ç‡: {summary['sessions_with_tokens']}/{summary['total_sessions']} ({summary['sessions_with_tokens']/summary['total_sessions']*100:.1f}%)")
    print(f"  â€¢ Token-Logger: {'âœ…' if summary['token_logger_enabled'] else 'âŒ'}")
    print(f"  â€¢ Token æ•°æ®åº“: {'âœ…' if summary['token_usage_exists'] else 'âŒ'}")
    print(f"  â€¢ ç¼ºå¤±æä¾›å•†: {summary['missing_providers']}")
    print(f"  â€¢ å‘ç°é—®é¢˜æ•°: {summary['issues_count']}")

if __name__ == "__main__":
    main()