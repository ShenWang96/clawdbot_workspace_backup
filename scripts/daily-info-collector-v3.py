#!/usr/bin/env python3
"""
æ¯æ—¥ä¿¡æ¯æ”¶é›†è„šæœ¬ - OpenClaw ç‰ˆæœ¬ V3
æŒ‰å››å¤§åˆ†ç±»ç»„ç»‡æ•°æ®æºï¼šæ”¿æ²»ã€ç»æµã€ç§‘æŠ€ã€ç¤¾äº¤åª’ä½“
æ¯ä¸ªåˆ†ç±»é…ç½®å¤šä¸ªæ•°æ®æº + å¤‡é€‰æ–¹æ¡ˆ
"""

import requests
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import os
import time
import re
import random
from typing import Dict, List, Any, Optional, Callable
from urllib.parse import urljoin


class DataValidator:
    """æ•°æ®æ ¡éªŒå™¨"""
    
    @staticmethod
    def is_valid_url(url: Optional[str]) -> bool:
        if not url:
            return False
        pattern = r'^https?://[^\s/$.?#].[^\s]*$'
        return bool(re.match(pattern, url))
    
    @staticmethod
    def validate_url(url: Optional[str]) -> Optional[str]:
        if url and DataValidator.is_valid_url(url):
            return url
        return None


class DailyInfoCollectorV3:
    """æ¯æ—¥ä¿¡æ¯æ”¶é›†å™¨ V3 - å››å¤§åˆ†ç±»æ¶æ„"""
    
    CATEGORIES = {
        'political': {'name': 'æ”¿æ²»', 'emoji': 'ğŸ›ï¸', 'priority': 1},
        'economic': {'name': 'ç»æµ', 'emoji': 'ğŸ’°', 'priority': 2},
        'tech': {'name': 'ç§‘æŠ€', 'emoji': 'ğŸ’»', 'priority': 3},
        'social': {'name': 'ç¤¾äº¤åª’ä½“', 'emoji': 'ğŸ“±', 'priority': 4}
    }
    
    # å…¨å±€è¶…æ—¶é…ç½®ï¼ˆç§’ï¼‰
    TIMEOUT_CONFIG = {
        'default': 10,      # é»˜è®¤è¶…æ—¶
        'rss': 10,          # RSSæºè¶…æ—¶
        'api': 15,          # APIè¶…æ—¶
        'slow_api': 20,     # æ…¢APIè¶…æ—¶
        'between_sources': 0.5,  # æ•°æ®æºé—´å»¶è¿Ÿ
    }
    
    def __init__(self, output_dir: str = None):
        self.date = datetime.now().strftime('%Y-%m-%d')
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = output_dir or f"/root/.openclaw/workspace/reports/daily-info/{self.date}"
        self.results = []
        self.valid_items = []
        self.validator = DataValidator()
        self.errors = []  # é”™è¯¯æ—¥å¿—
        
        # è¯·æ±‚ä¼šè¯é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/xml, application/rss+xml, text/html, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def log_error(self, source: str, error: str):
        """è®°å½•é”™è¯¯"""
        error_entry = {
            "source": source,
            "error": str(error),
            "time": datetime.now().isoformat()
        }
        self.errors.append(error_entry)
        print(f"  âŒ [{source}] ERROR: {str(error)[:80]}")
    
    def safe_request(self, url: str, headers: dict = None, timeout: int = None, method: str = 'get') -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼Œå¸¦è¶…æ—¶å’Œé”™è¯¯å¤„ç†"""
        timeout = timeout or self.TIMEOUT_CONFIG['default']
        headers = headers or {}
        
        try:
            if method.lower() == 'get':
                resp = self.session.get(url, headers=headers, timeout=timeout)
            else:
                resp = self.session.post(url, headers=headers, timeout=timeout)
            return resp
        except requests.Timeout:
            raise TimeoutError(f"Request timeout after {timeout}s")
        except requests.RequestException as e:
            raise ConnectionError(f"Request failed: {str(e)}")
    
    def run_collector_safely(self, collector_func, source_name: str) -> Dict:
        """å®‰å…¨è¿è¡Œæ”¶é›†å™¨ï¼Œç¡®ä¿å•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“"""
        try:
            result = collector_func()
            return result
        except Exception as e:
            self.log_error(source_name, str(e))
            return self._error_result(source_name, "unknown", source_name.lower().replace(' ', '_'), str(e))
    
    def prepare_output_directory(self):
        os.makedirs(self.output_dir, exist_ok=True)
    
    def normalize_item(self, title: str, url: str, source: str, category: str, 
                      platform: str, rank: int = 0, **kwargs) -> Optional[Dict]:
        """æ ‡å‡†åŒ–æ•°æ®é¡¹"""
        if not title or not url:
            return None
            
        return {
            "id": kwargs.get('id') or f"{platform}_{rank}_{int(time.time())}",
            "title": title[:200],  # é™åˆ¶é•¿åº¦
            "content": {
                "summary": kwargs.get('summary', '')[:500] if kwargs.get('summary') else None,
                "full_text": None
            },
            "media": {
                "image": {"url": self.validator.validate_url(kwargs.get('image_url')), "thumbnail": None},
                "video": None
            },
            "links": {
                "main": url,
                "mobile": self.validator.validate_url(kwargs.get('mobile_url')),
                "share": None
            },
            "metrics": {
                "hot": {
                    "value": kwargs.get('hot') or kwargs.get('score') or kwargs.get('stars'), 
                    "label": kwargs.get('metric_label', 'çƒ­åº¦')
                },
                "views": kwargs.get('views'),
                "interactions": {
                    "likes": kwargs.get('likes'),
                    "comments": kwargs.get('comments'),
                    "shares": kwargs.get('shares')
                }
            },
            "author": {
                "name": kwargs.get('author', '')[:50] if kwargs.get('author') else None,
                "avatar": self.validator.validate_url(kwargs.get('author_avatar'))
            },
            "time": {
                "published": kwargs.get('published_at'),
                "collected": datetime.now().isoformat()
            },
            "category": category,
            "tags": kwargs.get('tags', [])[:10],  # é™åˆ¶æ ‡ç­¾æ•°é‡
            "meta": {
                "source": source,
                "platform": platform,
                "category": category,
                "rank": rank,
                "collection_time": datetime.now().isoformat()
            }
        }
    
    def _success_result(self, source, category, platform, count, items):
        return {
            "source": source, "category": category, "platform": platform,
            "status": "success", "count": count, "valid_count": len(items), "items": items
        }
    
    def _error_result(self, source, category, platform, error):
        return {
            "source": source, "category": category, "platform": platform,
            "status": "error", "error": str(error), "count": 0, "valid_count": 0, "items": []
        }
    
    # ==================== æ”¿æ²»ç±»æ•°æ®æº ====================
    
    def fetch_reuters_news(self) -> Dict:
        """Reuters RSS æ–°é—»ï¼ˆå›½é™…æ”¿æ²»ç»æµï¼‰"""
        print("  [æ”¿æ²»] Fetching Reuters...", end=' ', flush=True)
        
        try:
            # Reuters RSS feeds
            feeds = [
                "https://www.reutersagency.com/feed/?taxonomy=markets&post_type=reuters-best",
                "https://www.reutersagency.com/feed/?taxonomy=tag:world&post_type=reuters-best"
            ]
            
            items = []
            for feed_url in feeds:
                try:
                    resp = self.safe_request(feed_url, timeout=self.TIMEOUT_CONFIG['rss'])
                    if resp and resp.status_code == 200:
                        root = ET.fromstring(resp.content)
                        channel = root.find('.//channel')
                        if channel is not None:
                            for item in channel.findall('item')[:10]:
                                title = item.findtext('title', '')
                                link = item.findtext('link', '')
                                desc = item.findtext('description', '')
                                pub_date = item.findtext('pubDate', '')
                                
                                if title and link:
                                    normalized = self.normalize_item(
                                        title=title, url=link, source="Reuters",
                                        category="political", platform="reuters",
                                        rank=len(items)+1, summary=desc,
                                        published_at=pub_date
                                    )
                                    if normalized:
                                        items.append(normalized)
                        break  # æˆåŠŸä¸€ä¸ªå°±è·³å‡º
                except Exception as e:
                    continue
            
            print(f"OK ({len(items)} items)")
            return self._success_result("Reuters", "political", "reuters", len(items), items)
            
        except Exception as e:
            self.log_error("Reuters", str(e))
            return self._error_result("Reuters", "political", "reuters", str(e))
    
    def fetch_bbc_news(self) -> Dict:
        """BBC News RSS"""
        print("  [æ”¿æ²»] Fetching BBC News...", end=' ', flush=True)
        
        try:
            url = "http://feeds.bbci.co.uk/news/world/rss.xml"
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['rss'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("BBC News", "political", "bbc", f"HTTP {resp.status_code if resp else 'No response'}")
            
            root = ET.fromstring(resp.content)
            items = []
            
            for item in root.findall('.//item')[:15]:
                title = item.findtext('title', '')
                link = item.findtext('link', '')
                desc = item.findtext('description', '')
                
                if title and link:
                    normalized = self.normalize_item(
                        title=title, url=link, source="BBC News",
                        category="political", platform="bbc",
                        rank=len(items)+1, summary=desc
                    )
                    if normalized:
                        items.append(normalized)
            
            print(f"OK ({len(items)} items)")
            return self._success_result("BBC News", "political", "bbc", len(items), items)
            
        except Exception as e:
            self.log_error("BBC News", str(e))
            return self._error_result("BBC News", "political", "bbc", str(e))
    
    # ==================== ç»æµç±»æ•°æ®æº ====================
    
    def fetch_yahoo_finance(self) -> Dict:
        """Yahoo Finance å¸‚åœºæ–°é—»ï¼ˆé€šè¿‡ RSSï¼‰"""
        print("  [ç»æµ] Fetching Yahoo Finance...", end=' ', flush=True)
        
        try:
            url = "https://finance.yahoo.com/news/rssindex"
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['rss'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("Yahoo Finance", "economic", "yahoo_finance", f"HTTP {resp.status_code if resp else 'No response'}")
            
            root = ET.fromstring(resp.content)
            items = []
            
            for item in root.findall('.//item')[:15]:
                title = item.findtext('title', '')
                link = item.findtext('link', '')
                desc = item.findtext('description', '')
                
                if title and link:
                    normalized = self.normalize_item(
                        title=title, url=link, source="Yahoo Finance",
                        category="economic", platform="yahoo_finance",
                        rank=len(items)+1, summary=desc
                    )
                    if normalized:
                        items.append(normalized)
            
            print(f"OK ({len(items)} items)")
            return self._success_result("Yahoo Finance", "economic", "yahoo_finance", len(items), items)
            
        except Exception as e:
            self.log_error("Yahoo Finance", str(e))
            return self._error_result("Yahoo Finance", "economic", "yahoo_finance", str(e))
    
    def fetch_cnbc_finance(self) -> Dict:
        """CNBC è´¢ç»æ–°é—»"""
        print("  [ç»æµ] Fetching CNBC...", end=' ', flush=True)
        
        try:
            url = "https://www.cnbc.com/id/100003114/device/rss/rss.html"
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['rss'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("CNBC", "economic", "cnbc", f"HTTP {resp.status_code if resp else 'No response'}")
            
            root = ET.fromstring(resp.content)
            items = []
            
            for item in root.findall('.//item')[:15]:
                title = item.findtext('title', '')
                link = item.findtext('link', '')
                desc = item.findtext('description', '')
                
                if title and link:
                    normalized = self.normalize_item(
                        title=title, url=link, source="CNBC",
                        category="economic", platform="cnbc",
                        rank=len(items)+1, summary=desc
                    )
                    if normalized:
                        items.append(normalized)
            
            print(f"OK ({len(items)} items)")
            return self._success_result("CNBC", "economic", "cnbc", len(items), items)
            
        except Exception as e:
            self.log_error("CNBC", str(e))
            return self._error_result("CNBC", "economic", "cnbc", str(e))
    
    # ==================== ç§‘æŠ€ç±»æ•°æ®æº ====================
    
    def fetch_github_trending(self) -> Dict:
        """GitHub Trending"""
        print("  [ç§‘æŠ€] Fetching GitHub Trending...", end=' ', flush=True)
        
        try:
            one_week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            url = f"https://api.github.com/search/repositories?q=created:>{one_week_ago}&sort=stars&order=desc&per_page=20"
            
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['slow_api'])
            if not resp or resp.status_code != 200:
                return self._error_result("GitHub", "tech", "github", f"HTTP {resp.status_code if resp else 'No response'}")
            
            data = resp.json()
            items = []
            
            for rank, repo in enumerate(data.get('items', [])[:20], 1):
                normalized = self.normalize_item(
                    title=repo.get('name', ''),
                    url=repo.get('html_url', ''),
                    source="GitHub Trending",
                    category="tech", platform="github",
                    rank=rank, id=str(repo.get('id')),
                    summary=repo.get('description'),
                    author=repo.get('owner', {}).get('login'),
                    author_avatar=repo.get('owner', {}).get('avatar_url'),
                    stars=repo.get('stargazers_count'),
                    metric_label="Stars",
                    tags=repo.get('topics', []),
                    published_at=repo.get('created_at')
                )
                if normalized:
                    items.append(normalized)
            
            print(f"OK ({len(items)} repos)")
            return self._success_result("GitHub", "tech", "github", len(items), items)
            
        except Exception as e:
            self.log_error("GitHub", str(e))
            return self._error_result("GitHub", "tech", "github", str(e))
    
    def fetch_hackernews(self) -> Dict:
        """Hacker News"""
        print("  [ç§‘æŠ€] Fetching Hacker News...", end=' ', flush=True)
        
        try:
            top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            resp = self.safe_request(top_url, timeout=self.TIMEOUT_CONFIG['api'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("Hacker News", "tech", "hackernews", f"HTTP {resp.status_code if resp else 'No response'}")
            
            story_ids = resp.json()[:20]
            items = []
            
            for rank, story_id in enumerate(story_ids, 1):
                try:
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_resp = self.safe_request(story_url, timeout=8)
                    
                    if story_resp and story_resp.status_code == 200:
                        story = story_resp.json()
                        if story:
                            normalized = self.normalize_item(
                                title=story.get('title', ''),
                                url=story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                                source="Hacker News",
                                category="tech", platform="hackernews",
                                rank=rank, id=str(story_id),
                                author=story.get('by'),
                                score=story.get('score'),
                                metric_label="Points",
                                comments=story.get('descendants')
                            )
                            if normalized:
                                items.append(normalized)
                except:
                    continue
            
            print(f"OK ({len(items)} stories)")
            return self._success_result("Hacker News", "tech", "hackernews", len(items), items)
            
        except Exception as e:
            self.log_error("Hacker News", str(e))
            return self._error_result("Hacker News", "tech", "hackernews", str(e))
    
    def fetch_techcrunch(self) -> Dict:
        """TechCrunch RSS"""
        print("  [ç§‘æŠ€] Fetching TechCrunch...", end=' ', flush=True)
        
        try:
            url = "https://techcrunch.com/feed/"
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['rss'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("TechCrunch", "tech", "techcrunch", f"HTTP {resp.status_code if resp else 'No response'}")
            
            root = ET.fromstring(resp.content)
            items = []
            
            for item in root.findall('.//item')[:15]:
                title = item.findtext('title', '')
                link = item.findtext('link', '')
                desc = item.findtext('description', '')
                
                if title and link:
                    normalized = self.normalize_item(
                        title=title, url=link, source="TechCrunch",
                        category="tech", platform="techcrunch",
                        rank=len(items)+1, summary=desc
                    )
                    if normalized:
                        items.append(normalized)
            
            print(f"OK ({len(items)} items)")
            return self._success_result("TechCrunch", "tech", "techcrunch", len(items), items)
            
        except Exception as e:
            self.log_error("TechCrunch", str(e))
            return self._error_result("TechCrunch", "tech", "techcrunch", str(e))
    
    def fetch_devto(self) -> Dict:
        """Dev.to çƒ­é—¨æ–‡ç« """
        print("  [ç§‘æŠ€] Fetching Dev.to...", end=' ', flush=True)
        
        try:
            url = "https://dev.to/api/articles?per_page=20&top=7"
            resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['api'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("Dev.to", "tech", "devto", f"HTTP {resp.status_code if resp else 'No response'}")
            
            articles = resp.json()
            items = []
            
            for rank, article in enumerate(articles[:20], 1):
                normalized = self.normalize_item(
                    title=article.get('title', ''),
                    url=article.get('url', ''),
                    source="Dev.to",
                    category="tech", platform="devto",
                    rank=rank, id=str(article.get('id')),
                    summary=article.get('description'),
                    author=article.get('user', {}).get('name'),
                    author_avatar=article.get('user', {}).get('profile_image'),
                    likes=article.get('positive_reactions_count'),
                    comments=article.get('comments_count'),
                    tags=article.get('tag_list', []),
                    published_at=article.get('published_at')
                )
                if normalized:
                    items.append(normalized)
            
            print(f"OK ({len(items)} articles)")
            return self._success_result("Dev.to", "tech", "devto", len(items), items)
            
        except Exception as e:
            self.log_error("Dev.to", str(e))
            return self._error_result("Dev.to", "tech", "devto", str(e))
    
    # ==================== ç¤¾äº¤åª’ä½“ç±»æ•°æ®æº ====================
    
    def fetch_reddit_hot(self) -> Dict:
        """Reddit çƒ­é—¨ï¼ˆr/technology, r/worldnewsï¼‰"""
        print("  [ç¤¾äº¤] Fetching Reddit...", end=' ', flush=True)
        
        try:
            # Reddit JSON APIï¼ˆæ— éœ€è®¤è¯ï¼Œä½†æœ‰é™åˆ¶ï¼‰
            subreddits = ['technology', 'worldnews', 'programming']
            items = []
            
            for subreddit in subreddits:
                try:
                    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                    resp = self.safe_request(url, timeout=self.TIMEOUT_CONFIG['api'])
                    
                    if resp and resp.status_code == 200:
                        data = resp.json()
                        posts = data.get('data', {}).get('children', [])
                        
                        for rank, post in enumerate(posts[:10], 1):
                            post_data = post.get('data', {})
                            title = post_data.get('title', '')
                            permalink = post_data.get('permalink', '')
                            url_link = post_data.get('url', '')
                            
                            # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨é“¾æ¥ï¼Œå¦åˆ™ç”¨ Reddit é“¾æ¥
                            link = url_link if url_link and not url_link.startswith('/r/') else f"https://reddit.com{permalink}"
                            
                            if title:
                                normalized = self.normalize_item(
                                    title=title, url=link, source=f"Reddit r/{subreddit}",
                                    category="social", platform=f"reddit_{subreddit}",
                                    rank=len(items)+1, id=post_data.get('id'),
                                    summary=post_data.get('selftext', '')[:200] if post_data.get('selftext') else None,
                                    author=post_data.get('author'),
                                    score=post_data.get('score'),
                                    metric_label="Upvotes",
                                    comments=post_data.get('num_comments')
                                )
                                if normalized:
                                    items.append(normalized)
                except Exception as e:
                    continue
            
            print(f"OK ({len(items)} posts)")
            return self._success_result("Reddit", "social", "reddit", len(items), items)
            
        except Exception as e:
            self.log_error("Reddit", str(e))
            return self._error_result("Reddit", "social", "reddit", str(e))
    
    # ==================== å›½å†…ç¤¾äº¤åª’ä½“æ•°æ®æº ====================
    
    def fetch_zhihu_hot(self) -> Dict:
        """çŸ¥ä¹çƒ­æ¦œ - ä½¿ç”¨ Cookie è®¤è¯"""
        print("  [ç¤¾äº¤] Fetching çŸ¥ä¹çƒ­æ¦œ...", end=' ', flush=True)
        
        try:
            url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Referer': 'https://www.zhihu.com/hot',
                'Cookie': 'z_c0=2|1:0|10:1770253111|4:z_c0|92:Mi4xQjllU0F3QUFBQUItTlpUWlFsZV9HeVlBQUFCZ0FsVk5OVGx4YWdEMll4bEYxWlNFbklqdGVSZE5YbzlKSVZwYkp3|d70fb725c9b1bc42d29f655cf8595703d352c758fec7e87da45213f2e1ceebfe'
            }
            
            resp = self.safe_request(url, headers=headers, timeout=self.TIMEOUT_CONFIG['api'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("çŸ¥ä¹çƒ­æ¦œ", "social", "zhihu", f"HTTP {resp.status_code if resp else 'No response'}")
            
            data = resp.json()
            items = []
            
            hot_list = data.get('data', [])
            for rank, item in enumerate(hot_list[:20], 1):
                try:
                    target = item.get('target', {})
                    title = target.get('title', '')
                    question_id = target.get('id', '')
                    
                    # æ„å»ºçŸ¥ä¹é—®é¢˜é“¾æ¥
                    if question_id:
                        link = f"https://www.zhihu.com/question/{question_id}"
                    else:
                        link = target.get('url', '')
                    
                    # è·å–çƒ­åº¦ä¿¡æ¯
                    detail_text = item.get('detail_text', '')  # å¦‚ "1234 ä¸‡çƒ­åº¦"
                    hot_value = None
                    if detail_text:
                        match = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡', detail_text)
                        if match:
                            hot_value = int(float(match.group(1)) * 10000)
                    
                    if title and link:
                        normalized = self.normalize_item(
                            title=title,
                            url=link,
                            source="çŸ¥ä¹çƒ­æ¦œ",
                            category="social",
                            platform="zhihu",
                            rank=rank,
                            id=str(question_id),
                            hot=hot_value,
                            metric_label="çƒ­åº¦",
                            summary=item.get('excerpt', '')
                        )
                        if normalized:
                            items.append(normalized)
                except Exception as e:
                    continue
            
            print(f"OK ({len(items)} items)")
            return self._success_result("çŸ¥ä¹çƒ­æ¦œ", "social", "zhihu", len(items), items)
            
        except Exception as e:
            self.log_error("çŸ¥ä¹çƒ­æ¦œ", str(e))
            return self._error_result("çŸ¥ä¹çƒ­æ¦œ", "social", "zhihu", str(e))
    
    def fetch_bilibili_hot(self) -> Dict:
        """Bç«™çƒ­é—¨æ’è¡Œæ¦œï¼ˆå…¨ç«™ï¼‰"""
        print("  [ç¤¾äº¤] Fetching Bç«™çƒ­é—¨...", end=' ', flush=True)
        
        try:
            # Bç«™å…¨ç«™çƒ­é—¨æ’è¡Œæ¦œ
            url = "https://api.bilibili.com/x/web-interface/popular"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/plain, */*',
                'Referer': 'https://www.bilibili.com'
            }
            
            resp = self.safe_request(url, headers=headers, timeout=self.TIMEOUT_CONFIG['api'])
            
            if not resp or resp.status_code != 200:
                return self._error_result("Bç«™çƒ­é—¨", "social", "bilibili", f"HTTP {resp.status_code if resp else 'No response'}")
            
            data = resp.json()
            items = []
            
            if data.get('code') != 0:
                return self._error_result("Bç«™çƒ­é—¨", "social", "bilibili", f"API Error: {data.get('message', 'Unknown')}")
            
            video_list = data.get('data', {}).get('list', [])
            for rank, video in enumerate(video_list[:20], 1):
                try:
                    title = video.get('title', '')
                    bvid = video.get('bvid', '')
                    link = f"https://www.bilibili.com/video/{bvid}" if bvid else video.get('short_link', '')
                    
                    owner = video.get('owner', {})
                    author = owner.get('name', '')
                    
                    stat = video.get('stat', {})
                    views = stat.get('view', 0)
                    likes = stat.get('like', 0)
                    comments = stat.get('reply', 0)
                    
                    # å°é¢å›¾
                    cover = video.get('pic', '')
                    
                    if title and link:
                        normalized = self.normalize_item(
                            title=title,
                            url=link,
                            source="Bç«™çƒ­é—¨",
                            category="social",
                            platform="bilibili",
                            rank=rank,
                            id=str(video.get('aid', '')),
                            views=views,
                            likes=likes,
                            comments=comments,
                            metric_label="æ’­æ”¾é‡",
                            author=author,
                            image_url=cover,
                            summary=video.get('rcmd_reason', {}).get('content', '')
                        )
                        if normalized:
                            items.append(normalized)
                except Exception as e:
                    continue
            
            print(f"OK ({len(items)} videos)")
            return self._success_result("Bç«™çƒ­é—¨", "social", "bilibili", len(items), items)
            
        except Exception as e:
            self.log_error("Bç«™çƒ­é—¨", str(e))
            return self._error_result("Bç«™çƒ­é—¨", "social", "bilibili", str(e))
    
    # ==================== ä¸»æ§é€»è¾‘ ====================
    
    def collect_all(self):
        """æŒ‰åˆ†ç±»æ”¶é›†æ‰€æœ‰æ•°æ®æº"""
        self.prepare_output_directory()
        
        print(f"\n{'='*70}")
        print(f"ğŸ“° Daily Info Collector V3 - {self.date}")
        print(f"   æ”¿æ²» | ç»æµ | ç§‘æŠ€ | ç¤¾äº¤åª’ä½“")
        print(f"   è¶…æ—¶é…ç½®: RSS={self.TIMEOUT_CONFIG['rss']}s, API={self.TIMEOUT_CONFIG['api']}s")
        print(f"{'='*70}\n")
        
        # æŒ‰åˆ†ç±»å®šä¹‰æ•°æ®æºï¼ˆå¸¦ä¼˜å…ˆçº§å’Œåç§°æ˜ å°„ï¼‰
        collectors_by_category = {
            'political': [
                (self.fetch_bbc_news, "BBC News"),
                (self.fetch_reuters_news, "Reuters"),
            ],
            'economic': [
                (self.fetch_yahoo_finance, "Yahoo Finance"),
                (self.fetch_cnbc_finance, "CNBC"),
            ],
            'tech': [
                (self.fetch_github_trending, "GitHub"),
                (self.fetch_hackernews, "Hacker News"),
                (self.fetch_techcrunch, "TechCrunch"),
                (self.fetch_devto, "Dev.to"),
            ],
            'social': [
                (self.fetch_zhihu_hot, "çŸ¥ä¹çƒ­æ¦œ"),
                (self.fetch_bilibili_hot, "Bç«™çƒ­é—¨"),
                (self.fetch_reddit_hot, "Reddit"),
            ]
        }
        
        # æ‰§è¡Œæ”¶é›†
        for category, collectors in collectors_by_category.items():
            cat_info = self.CATEGORIES.get(category, {})
            emoji = cat_info.get('emoji', 'ğŸ“„')
            name = cat_info.get('name', category)
            
            print(f"\n{emoji} [{name}] å¼€å§‹æ”¶é›†...")
            
            for collector_func, source_name in collectors:
                # ä½¿ç”¨å®‰å…¨åŒ…è£…å™¨è¿è¡Œï¼Œç¡®ä¿å•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“
                result = self.run_collector_safely(collector_func, source_name)
                self.results.append(result)
                if result['status'] == 'success':
                    self.valid_items.extend(result.get('items', []))
                time.sleep(self.TIMEOUT_CONFIG['between_sources'])  # æ•°æ®æºé—´å»¶è¿Ÿ
        
        return self.save_and_summarize()
    
    def save_and_summarize(self):
        """ä¿å­˜ç»“æœå¹¶ç”Ÿæˆæ±‡æ€»"""
        total_items = len(self.valid_items)
        successful = len([r for r in self.results if r['status'] == 'success'])
        failed = len([r for r in self.results if r['status'] == 'error'])
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for cat_key, cat_info in self.CATEGORIES.items():
            cat_items = [i for i in self.valid_items if i.get('category') == cat_key]
            sources = list(set(i.get('meta', {}).get('source', '') for i in cat_items))
            category_stats[cat_key] = {
                'name': cat_info['name'],
                'emoji': cat_info['emoji'],
                'count': len(cat_items),
                'sources': sources
            }
        
        output_data = {
            "schema_version": "3.0",
            "date": self.date,
            "collection_time": datetime.now().isoformat(),
            "summary": {
                "total_items": total_items,
                "total_sources": len(self.results),
                "successful_sources": successful,
                "failed_sources": failed,
                "categories": category_stats
            },
            "items": self.valid_items
        }
        
        # ä¿å­˜æ–‡ä»¶
        detail_file = os.path.join(self.output_dir, f"daily_info_{self.timestamp}.json")
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        latest_json = os.path.join(self.output_dir, "latest.json")
        with open(latest_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        md_content = self.generate_markdown_report(output_data)
        md_file = os.path.join(self.output_dir, f"daily_info_{self.timestamp}.md")
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        latest_md = os.path.join(self.output_dir, "latest.md")
        with open(latest_md, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # æ‰“å°æ±‡æ€»
        print(f"\n{'='*70}")
        print(f"âœ… æ”¶é›†å®Œæˆæ±‡æ€»")
        print(f"{'='*70}")
        print(f"æ€»æ¡ç›®: {total_items} | æˆåŠŸæº: {successful}/{len(self.results)} | å¤±è´¥: {failed}")
        print(f"\næŒ‰åˆ†ç±»ç»Ÿè®¡:")
        for cat_key, stats in category_stats.items():
            print(f"  {stats['emoji']} {stats['name']}: {stats['count']} æ¡")
        
        # æ˜¾ç¤ºé”™è¯¯æ±‡æ€»ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.errors:
            print(f"\nâš ï¸  å¤±è´¥æºè¯¦æƒ…:")
            for err in self.errors[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé”™è¯¯
                print(f"  - {err['source']}: {err['error'][:60]}...")
        
        print(f"{'='*70}\n")
        
        return {
            "date": self.date,
            "total_items": total_items,
            "successful_sources": successful,
            "failed_sources": failed,
            "output_files": {
                "json": detail_file,
                "markdown": md_file,
                "latest_json": latest_json,
                "latest_md": latest_md
            },
            "summary": output_data["summary"]
        }
    
    def generate_markdown_report(self, data: Dict) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        lines = [
            f"# ğŸ“° æ¯æ—¥ä¿¡æ¯æ”¶é›†æŠ¥å‘Š - {self.date}",
            "",
            f"**æ”¶é›†æ—¶é—´**: {data['collection_time']}",
            f"**æ•°æ®ç‰ˆæœ¬**: Schema {data['schema_version']}",
            "",
            "## ğŸ“Š æ±‡æ€»ç»Ÿè®¡",
            "",
            f"- **æ€»æ¡ç›®æ•°**: {data['summary']['total_items']}",
            f"- **æ•°æ®æº**: {data['summary']['successful_sources']}/{data['summary']['total_sources']} æˆåŠŸ",
            "",
            "### æŒ‰åˆ†ç±»ç»Ÿè®¡",
            ""
        ]
        
        # åˆ†ç±»ç»Ÿè®¡
        for cat_key, stats in data['summary']['categories'].items():
            lines.append(f"- {stats['emoji']} **{stats['name']}**: {stats['count']} æ¡")
        
        # å„åˆ†ç±»è¯¦ç»†å†…å®¹
        lines.append("")
        
        for cat_key in ['political', 'economic', 'tech', 'social']:
            cat_stats = data['summary']['categories'].get(cat_key, {})
            cat_name = cat_stats.get('name', cat_key)
            cat_emoji = cat_stats.get('emoji', 'ğŸ“„')
            cat_items = [i for i in data['items'] if i.get('category') == cat_key]
            
            if not cat_items:
                continue
            
            lines.extend([f"## {cat_emoji} {cat_name}", ""])
            
            # æŒ‰æ¥æºåˆ†ç»„
            sources = {}
            for item in cat_items:
                src = item.get('meta', {}).get('source', 'Unknown')
                if src not in sources:
                    sources[src] = []
                sources[src].append(item)
            
            for src_name, src_items in sources.items():
                lines.extend([f"### {src_name}", ""])
                for i, item in enumerate(src_items[:10], 1):  # æ¯æºæœ€å¤š10æ¡
                    title = item.get('title', 'N/A')
                    link = item.get('links', {}).get('main', '')
                    score = item.get('metrics', {}).get('hot', {}).get('value')
                    score_str = f" ğŸ”¥{score}" if score else ""
                    
                    if link:
                        lines.append(f"{i}. [{title}]({link}){score_str}")
                    else:
                        lines.append(f"{i}. {title}{score_str}")
                lines.append("")
        
        lines.extend(["---", "", "*Generated by OpenClaw Daily Info Collector V3*"])
        return "\n".join(lines)


def main():
    collector = DailyInfoCollectorV3()
    result = collector.collect_all()
    
    # è¾“å‡ºç®€è¦æ±‡æŠ¥
    print("\n" + "="*70)
    print("BRIEF_REPORT_START")
    print(json.dumps({
        "date": result["date"],
        "total_items": result["total_items"],
        "sources_success": result["successful_sources"],
        "sources_failed": result["failed_sources"],
        "categories": {k: v['count'] for k, v in result["summary"]["categories"].items()},
        "output_dir": collector.output_dir
    }, ensure_ascii=False, indent=2))
    print("BRIEF_REPORT_END")
    print("="*70)


if __name__ == "__main__":
    main()
